###############################################################################
#  Copyright (C) 2024 LiveTalking@lipku https://github.com/lipku/LiveTalking
#  email: lipku@foxmail.com
# 
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  
#       http://www.apache.org/licenses/LICENSE-2.0
# 
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
###############################################################################

import torch
import numpy as np

import os
import time
import cv2
import glob
import pickle
import copy

import queue
from queue import Queue
from threading import Thread, Event
import torch.multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor, as_completed

from tqdm import tqdm
from src.basereal import BaseReal
from src.wav2lip.models import Wav2Lip
from src.lipasr import LipASR
from src.log import logger

pwd_path = os.path.dirname(os.path.abspath(__file__))
root_dir = os.path.dirname(pwd_path)

device = "cuda" if torch.cuda.is_available() else (
    "mps" if (hasattr(torch.backends, "mps") and torch.backends.mps.is_available()) else "cpu")

# 全局avatar缓存
_avatar_cache = {}


def _load(checkpoint_path):
    if device == 'cuda':
        checkpoint = torch.load(checkpoint_path)  # ,weights_only=True
    else:
        checkpoint = torch.load(checkpoint_path,
                                map_location=lambda storage, loc: storage)
    return checkpoint


def load_model(path):
    model = Wav2Lip()
    logger.info(f"Load checkpoint from: {path}")
    checkpoint = _load(path)
    s = checkpoint["state_dict"]
    new_s = {}
    for k, v in s.items():
        new_s[k.replace('module.', '')] = v
    model.load_state_dict(new_s)
    logger.info(f"device: {device}")
    model = model.to(device)
    return model.eval()


def load_avatar(avatar_id):
    """加载avatar数据，带缓存机制"""
    global _avatar_cache
    
    # 检查缓存
    if avatar_id in _avatar_cache:
        logger.info(f'Avatar "{avatar_id}" loaded from cache')
        return _avatar_cache[avatar_id]
    
    logger.info(f'Loading avatar "{avatar_id}" from disk...')
    avatar_path = os.path.join(root_dir, 'data', avatar_id)
    full_imgs_path = f"{avatar_path}/full_imgs"
    face_imgs_path = f"{avatar_path}/face_imgs"
    coords_path = f"{avatar_path}/coords.pkl"

    with open(coords_path, 'rb') as f:
        coord_list_cycle = pickle.load(f)
    input_img_list = glob.glob(os.path.join(full_imgs_path, '*.[jpJP][pnPN]*[gG]'))
    input_img_list = sorted(input_img_list, key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))
    frame_list_cycle = read_imgs(input_img_list)
    input_face_list = glob.glob(os.path.join(face_imgs_path, '*.[jpJP][pnPN]*[gG]'))
    input_face_list = sorted(input_face_list, key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))
    face_list_cycle = read_imgs(input_face_list)

    # 缓存结果
    _avatar_cache[avatar_id] = (frame_list_cycle, face_list_cycle, coord_list_cycle)
    logger.info(f'Avatar "{avatar_id}" loaded and cached (full: {len(frame_list_cycle)}, face: {len(face_list_cycle)})')
    
    return frame_list_cycle, face_list_cycle, coord_list_cycle


def preload_avatars(avatar_ids):
    """预加载多个avatar到缓存"""
    logger.info(f'Preloading {len(avatar_ids)} avatars...')
    for avatar_id in avatar_ids:
        if avatar_id not in _avatar_cache:
            try:
                load_avatar(avatar_id)
            except Exception as e:
                logger.error(f'Failed to preload avatar "{avatar_id}": {e}')
    logger.info(f'Preloading completed. Cached avatars: {list(_avatar_cache.keys())}')


def clear_avatar_cache(avatar_id=None):
    """清除avatar缓存
    
    Args:
        avatar_id: 指定要清除的avatar_id，如果为None则清除所有
    """
    global _avatar_cache
    if avatar_id is None:
        _avatar_cache.clear()
        logger.info('All avatar cache cleared')
    elif avatar_id in _avatar_cache:
        del _avatar_cache[avatar_id]
        logger.info(f'Avatar "{avatar_id}" cache cleared')


def get_cached_avatars():
    """获取已缓存的avatar列表"""
    return list(_avatar_cache.keys())


@torch.no_grad()
def warm_up(batch_size, model, modelres):
    # 预热函数
    logger.info('warmup model...')
    img_batch = torch.ones(batch_size, 6, modelres, modelres).to(device)
    mel_batch = torch.ones(batch_size, 1, 80, 16).to(device)
    model(mel_batch, img_batch)


def read_imgs(img_list, max_workers=8):
    """并发读取图像列表，提升加载速度
    
    Args:
        img_list: 图像文件路径列表
        max_workers: 最大并发线程数，默认8
        
    Returns:
        成功读取的图像帧列表（保持原始顺序）
    """
    def load_single_image(img_path):
        """加载单张图像"""
        try:
            frame = cv2.imread(img_path)
            if frame is None:
                logger.warning(f"Failed to load image: {img_path}")
                return None
            return frame
        except Exception as e:
            logger.error(f"Error loading image {img_path}: {e}")
            return None
    
    frames = [None] * len(img_list)
    failed_count = 0
    
    logger.info(f'Reading {len(img_list)} images with {max_workers} workers...')
    start_time = time.perf_counter()
    
    # 使用线程池并发读取图像
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # 提交所有任务，保存索引
        future_to_index = {
            executor.submit(load_single_image, img_path): idx 
            for idx, img_path in enumerate(img_list)
        }
        
        # 使用tqdm显示进度
        with tqdm(total=len(img_list), desc="Loading images") as pbar:
            for future in as_completed(future_to_index):
                idx = future_to_index[future]
                try:
                    frame = future.result()
                    if frame is not None:
                        frames[idx] = frame
                    else:
                        failed_count += 1
                except Exception as e:
                    logger.error(f"Unexpected error for image {idx}: {e}")
                    failed_count += 1
                pbar.update(1)
    
    # 过滤掉失败的图像
    frames = [f for f in frames if f is not None]
    
    elapsed = time.perf_counter() - start_time
    logger.info(f'Loaded {len(frames)} images in {elapsed:.2f}s ({len(frames)/elapsed:.1f} imgs/s)')
    
    if failed_count > 0:
        logger.warning(f'Failed to load {failed_count} images')
    
    if len(frames) == 0:
        raise ValueError("No images were successfully loaded")
    
    return frames


def __mirror_index(size, index):
    # size = len(self.coord_list_cycle)
    turn = index // size
    res = index % size
    if turn % 2 == 0:
        return res
    else:
        return size - res - 1


def inference(quit_event, batch_size, face_list_cycle, audio_feat_queue, audio_out_queue, res_frame_queue, model):
    length = len(face_list_cycle)
    index = 0
    count = 0
    counttime = 0
    logger.debug('start inference')
    while not quit_event.is_set():
        try:
            mel_batch = audio_feat_queue.get(block=True, timeout=1)
        except queue.Empty:
            continue

        is_all_silence = True
        audio_frames = []
        for _ in range(batch_size * 2):
            frame, type, eventpoint = audio_out_queue.get()
            audio_frames.append((frame, type, eventpoint))
            if type == 0:
                is_all_silence = False

        if is_all_silence:
            for i in range(batch_size):
                res_frame_queue.put((None, __mirror_index(length, index), audio_frames[i * 2:i * 2 + 2]))
                index = index + 1
        else:
            t = time.perf_counter()
            img_batch = []
            for i in range(batch_size):
                idx = __mirror_index(length, index + i)
                face = face_list_cycle[idx]
                img_batch.append(face)
            img_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)

            img_masked = img_batch.copy()
            img_masked[:, face.shape[0] // 2:] = 0

            img_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.
            mel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])

            img_batch = torch.FloatTensor(np.transpose(img_batch, (0, 3, 1, 2))).to(device)
            mel_batch = torch.FloatTensor(np.transpose(mel_batch, (0, 3, 1, 2))).to(device)

            with torch.no_grad():
                pred = model(mel_batch, img_batch)
            pred = pred.cpu().numpy().transpose(0, 2, 3, 1) * 255.

            counttime += (time.perf_counter() - t)
            count += batch_size
            if count >= 1000:
                logger.info(f"actual avg infer fps:{count / counttime:.4f}")
                count = 0
                counttime = 0
            for i, res_frame in enumerate(pred):
                res_frame_queue.put((res_frame, __mirror_index(length, index), audio_frames[i * 2:i * 2 + 2]))
                index = index + 1
    logger.debug('lipreal inference processor stop')


class LipReal(BaseReal):
    @torch.no_grad()
    def __init__(self, opt, model, avatar):
        super().__init__(opt)
        # self.opt = opt # shared with the trainer's opt to support in-place modification of rendering parameters.
        # self.W = opt.W
        # self.H = opt.H

        self.fps = opt.fps  # 20 ms per frame

        self.batch_size = opt.batch_size
        self.idx = 0
        self.res_frame_queue = Queue(self.batch_size * 2)  # mp.Queue
        # self.__loadavatar()
        self.model = model
        self.frame_list_cycle, self.face_list_cycle, self.coord_list_cycle = avatar

        self.asr = LipASR(opt, self)
        self.asr.warm_up()

        self.render_event = mp.Event()

    def paste_back_frame(self, pred_frame, idx: int):
        bbox = self.coord_list_cycle[idx]
        combine_frame = copy.deepcopy(self.frame_list_cycle[idx])
        # combine_frame = copy.deepcopy(self.imagecache.get_img(idx))
        y1, y2, x1, x2 = bbox
        res_frame = cv2.resize(pred_frame.astype(np.uint8), (x2 - x1, y2 - y1))
        # combine_frame = get_image(ori_frame,res_frame,bbox)
        # t=time.perf_counter()
        combine_frame[y1:y2, x1:x2] = res_frame
        return combine_frame

    def render(self, quit_event, loop=None, audio_track=None, video_track=None):
        # if self.opt.asr:
        #     self.asr.warm_up()

        self.init_customindex()
        self.tts.render(quit_event)

        infer_quit_event = Event()
        infer_thread = Thread(target=inference, args=(infer_quit_event, self.batch_size, self.face_list_cycle,
                                                      self.asr.feat_queue, self.asr.output_queue, self.res_frame_queue,
                                                      self.model,))  # mp.Process
        infer_thread.start()

        process_quit_event = Event()
        process_thread = Thread(target=self.process_frames, args=(process_quit_event, loop, audio_track, video_track))
        process_thread.start()

        # self.render_event.set() #start infer process render
        count = 0
        totaltime = 0
        _starttime = time.perf_counter()
        # _totalframe=0
        while not quit_event.is_set():
            # update texture every frame
            # audio stream thread...
            t = time.perf_counter()
            self.asr.run_step()

            if video_track and video_track._queue.qsize() >= 5:
                # logger.debug(f'pausing production for queue control, queue size: {video_track._queue.qsize()}')
                time.sleep(0.04 * video_track._queue.qsize() * 0.8)

        logger.info('lipreal thread stop')

        infer_quit_event.set()
        infer_thread.join()

        process_quit_event.set()
        process_thread.join()
